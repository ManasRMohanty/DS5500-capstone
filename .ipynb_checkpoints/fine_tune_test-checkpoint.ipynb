{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "from transformers import BertModel\n",
    "import nltk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for p in bert_model.base_model.base_model.parameters():\n",
    "    i = i + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_available_in_pos_list(dict_list, begin_pos, end_pos):\n",
    "    for entry in dict_list:\n",
    "        if(entry[\"start\"]<=begin_pos and entry[\"end\"]>=end_pos):\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def get_bert_token_positions(input_text,token_list,start_from_pos=0):\n",
    "    \n",
    "    pos_list = []                    \n",
    "    \n",
    "    name_to_match = input_text.lower().replace(\" \",\"\")\n",
    "    remaining_name = input_text.lower().replace(\" \",\"\")\n",
    "    \n",
    "    name = \"\"\n",
    "    count = start_from_pos\n",
    "\n",
    "    for entry in token_list[start_from_pos:]:\n",
    "        if(remaining_name.startswith(entry.strip(\"##\").lower())):\n",
    "            pos_list.append(count)\n",
    "            remaining_name = remaining_name[len(entry.strip(\"##\").lower()):]\n",
    "            name = name + entry.strip(\"##\").lower()\n",
    "            if(name == name_to_match):\n",
    "                break\n",
    "        else:\n",
    "            pos_list = []\n",
    "            name = \"\"\n",
    "            remaining_name = name_to_match\n",
    "            if(remaining_name.startswith(entry.strip(\"##\").lower())):                                 \n",
    "                pos_list.append(count)                                                                \n",
    "                remaining_name = remaining_name[len(entry.strip(\"##\").lower()):]\n",
    "                name = name + entry.strip(\"##\").lower()    \n",
    "                if(name == name_to_match):                                   \n",
    "                    break\n",
    "\n",
    "        count = count + 1\n",
    "\n",
    "    return pos_list\n",
    "\n",
    "def process_string(string_input, entity_list):\n",
    "    \n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    sentences = tokenizer.tokenize(string_input)\n",
    "    \n",
    "    #config = BertConfig.from_pretrained('C:/Users/itsma/Documents/BERT_models/NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12')\n",
    "    #config.output_hidden_states = True\n",
    "\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained('C:/Users/itsma/Documents/BERT_models/NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12')\n",
    "    #bert_model = BertModel.from_pretrained(\"C:/Users/itsma/Documents/BERT_models/NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12\",config=config)\n",
    "    \n",
    "    positions_covered = 0\n",
    "    sentence_list = []\n",
    "    \n",
    "    for index in range(len(sentences)):\n",
    "        new_dict_sentence = {}\n",
    "        sentence = sentences[index]\n",
    "        new_dict_sentence[\"sentence\"] = sentence\n",
    "        #new_dict_sentence[\"padding_length\"] = padding_length\n",
    "        #start_index_bert = max(0,index-padding_length)\n",
    "        #end_index_bert = min(len(sentences),index+padding_length)\n",
    "\n",
    "        bert_input = sentences[index]\n",
    "\n",
    "        encodings = bert_tokenizer.encode(bert_input,add_special_tokens = True)\n",
    "        new_dict_sentence[\"encodings\"] = encodings\n",
    "        input_ids = torch.tensor(encodings).unsqueeze(0)  \n",
    "        outputs = bert_model(input_ids)\n",
    "        bert_vector = outputs[2]\n",
    "        bert_tokens = bert_tokenizer.convert_ids_to_tokens(encodings) #bert_tokenizer.tokenize(bert_input,add_special_tokens = True)\n",
    "        new_dict_sentence[\"tokens\"] = bert_tokens\n",
    "        \n",
    "        start_pos = 0\n",
    "        #prior_pos = get_bert_token_positions(' '.join(sentences[start_index_bert:index]),bert_tokens)\n",
    "        \n",
    "        #if(len(prior_pos)>0):\n",
    "            #start_pos = max(prior_pos)\n",
    "            \n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        pos_tokens = nltk.pos_tag(tokens)\n",
    "\n",
    "        sentence_covered = ''\n",
    "        \n",
    "        label_list = []\n",
    "        \n",
    "        for i in range(len(entity_list)):\n",
    "            label_list.append([0] * len(encodings))\n",
    "        \n",
    "        for token in pos_tokens:\n",
    "            new_dict = {}\n",
    "            current_word = token[0]\n",
    "            token_position = string_input.find(current_word, positions_covered)\n",
    "            spaces_between = string_input[positions_covered:token_position] \n",
    "            sentence_covered = sentence_covered + spaces_between + current_word\n",
    "            positions_covered = token_position + len(current_word)\n",
    "            begin_pos = token_position\n",
    "            end_pos = positions_covered\n",
    "\n",
    "            bert_token_positions = get_bert_token_positions(current_word,bert_tokens,start_pos)\n",
    "            \n",
    "            vec_list_layers = []\n",
    "            \n",
    "            if(len(bert_token_positions)==0):\n",
    "                continue\n",
    "            start_pos = bert_token_positions[-1] + 1\n",
    "            \n",
    "            entity_index=0\n",
    "            for entity in entity_list:\n",
    "                if(is_available_in_pos_list(entity,begin_pos,end_pos)):\n",
    "                    for entry in bert_token_positions:\n",
    "                        label_list[entity_index][entry] = 1\n",
    "                \n",
    "                entity_index = entity_index + 1\n",
    "                \n",
    "                \n",
    "        new_dict_sentence[\"labels\"] = label_list\n",
    "\n",
    "        sentence_list.append(new_dict_sentence)\n",
    "        \n",
    "    return sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DischargeNote():\n",
    "    def __init__(self, root,padding_length=0):\n",
    "        self.xml_root = root\n",
    "        self.padding_length = padding_length\n",
    "        \n",
    "    def process_note(self):\n",
    "        root = self.xml_root\n",
    "        text_section = root.find('TEXT')\n",
    "        text = text_section.text\n",
    "        \n",
    "        tag_section = root.find('TAGS')\n",
    "        event_list = []\n",
    "        timex_list = []\n",
    "        tlink_list = []\n",
    "        sectime_list = []\n",
    "        for child in tag_section:\n",
    "            if(child.tag=='EVENT'):\n",
    "                event_list.append(child.attrib)\n",
    "            elif(child.tag=='TIMEX3'):\n",
    "                timex_list.append(child.attrib)\n",
    "            elif(child.tag=='TLINK'):\n",
    "                tlink_list.append(child.attrib)\n",
    "            elif(child.tag=='SECTIME'):\n",
    "                sectime_list.append(child.attrib)\n",
    "        \n",
    "        event_list_new = []\n",
    "        \n",
    "        for sub in event_list:\n",
    "            new_dict = {}\n",
    "            new_dict[\"start\"] = int(sub[\"start\"])\n",
    "            new_dict[\"end\"] = int(sub[\"end\"])\n",
    "            event_list_new.append(new_dict)\n",
    "        \n",
    "        timex_list_new = []\n",
    "        \n",
    "        for sub in timex_list:\n",
    "            new_dict = {}\n",
    "            new_dict[\"start\"] = int(sub[\"start\"])\n",
    "            new_dict[\"end\"] = int(sub[\"end\"])\n",
    "            timex_list_new.append(new_dict)\n",
    "\n",
    "        for sub in sectime_list:\n",
    "            new_dict = {}\n",
    "            new_dict[\"start\"] = int(sub[\"start\"])\n",
    "            new_dict[\"end\"] = int(sub[\"end\"])\n",
    "            if not any((d['start'] == new_dict[\"start\"] and d['end'] == new_dict[\"end\"]) for d in timex_list_new):\n",
    "                timex_list_new.append(new_dict)\n",
    "        \n",
    "        entity_list = []\n",
    "        entity_list.append(event_list_new)\n",
    "        entity_list.append(timex_list_new)\n",
    "        \n",
    "        self.processed_text = process_string(text,entity_list)\n",
    "        \n",
    "        sentences_length = len(self.processed_text)\n",
    "        encoding_list = []\n",
    "        event_label_list = []\n",
    "        timex_label_list = []\n",
    "        token_list = []\n",
    "        \n",
    "        for sentence_index in range(sentences_length):\n",
    "            padding_unsuccess = True\n",
    "            padding_length = self.padding_length\n",
    "            \n",
    "            encodings = []\n",
    "            event_labels = []\n",
    "            timex_labels = []\n",
    "            tokens = []\n",
    "            while(padding_unsuccess):\n",
    "                encodings = []\n",
    "                event_labels = []\n",
    "                timex_labels = []\n",
    "                tokens=[]\n",
    "                \n",
    "                begin_index = max(0,sentence_index-padding_length)\n",
    "                end_index = min(sentences_length,sentence_index+padding_length)\n",
    "                current_index = 1\n",
    "                last_index = end_index - begin_index + 1\n",
    "        \n",
    "                if(begin_index==end_index):\n",
    "                    entry = self.processed_text[begin_index]\n",
    "                    encodings.extend(entry['encodings'])\n",
    "                    tokens.extend(entry['tokens'])\n",
    "                    event_labels.extend(entry['labels'][0])\n",
    "                    timex_labels.extend(entry['labels'][1])\n",
    "                else:\n",
    "                    for entry in self.processed_text[begin_index:end_index+1]:\n",
    "                        if(current_index==1):\n",
    "                            encodings.extend(entry['encodings'][:-1])\n",
    "                            tokens.extend(entry['tokens'][:-1])\n",
    "                            event_labels.extend(entry['labels'][0][:-1])\n",
    "                            timex_labels.extend(entry['labels'][1][:-1])\n",
    "                        elif(current_index==last_index):\n",
    "                            encodings.extend(entry['encodings'][1:])\n",
    "                            tokens.extend(entry['tokens'][1:])\n",
    "                            event_labels.extend(entry['labels'][0][1:])\n",
    "                            timex_labels.extend(entry['labels'][1][1:])\n",
    "                        else:\n",
    "                            encodings.extend(entry['encodings'][1:-1])\n",
    "                            tokens.extend(entry['tokens'][1:-1])\n",
    "                            event_labels.extend(entry['labels'][0][1:-1])\n",
    "                            timex_labels.extend(entry['labels'][1][1:-1])\n",
    "                        current_index = current_index + 1\n",
    "\n",
    "                if(len(encodings)<=512):\n",
    "                    padding_unsuccess = False\n",
    "                else:\n",
    "                    padding_length = max(0,padding_length - 1)\n",
    "            \n",
    "            encoding_list.append(encodings)\n",
    "            token_list.append(tokens)\n",
    "            event_label_list.append(event_labels)\n",
    "            timex_label_list.append(timex_labels)\n",
    "            \n",
    "        return [encoding_list,token_list,event_label_list,timex_label_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"1.xml\"\n",
    "\n",
    "file_name = os.path.join(\"C:/Users/itsma/Documents/Capstone project/DS5500-capstone/all_data/\", file)\n",
    "tree = ET.parse(file_name)\n",
    "root = tree.getroot()\n",
    "discharge_note = DischargeNote(root,1)\n",
    "results = discharge_note.process_note()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 9634, 3058, 1024, 5641, 1013, 2756, 1013, 2857, 11889, 3058, 1024, 2184, 1013, 5840, 1013, 2857, 2381, 1997, 2556, 7355, 1024, 1996, 5776, 2003, 1037, 2654, 1011, 2095, 1011, 2214, 2450, 2040, 2003, 9820, 3893, 2005, 2048, 2086, 1012, 2016, 3591, 2007, 2187, 3356, 29371, 3255, 2004, 2092, 2004, 19029, 1998, 24780, 2029, 2003, 1037, 2146, 1011, 3061, 12087, 1012, 2016, 2001, 11441, 1999, 2889, 2076, 1996, 4182, 1997, 2014, 2775, 1012, 102]\n"
     ]
    }
   ],
   "source": [
    "print(results[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'admission', 'date', ':', '09', '/', '29', '/', '1993', 'discharge', 'date', ':', '10', '/', '04', '/', '1993', 'history', 'of', 'present', 'illness', ':', 'the', 'patient', 'is', 'a', '28', '-', 'year', '-', 'old', 'woman', 'who', 'is', 'hiv', 'positive', 'for', 'two', 'years', '.', 'she', 'presented', 'with', 'left', 'upper', 'quadrant', 'pain', 'as', 'well', 'as', 'nausea', 'and', 'vomiting', 'which', 'is', 'a', 'long', '-', 'standing', 'complaint', '.', 'she', 'was', 'diagnosed', 'in', '1991', 'during', 'the', 'birth', 'of', 'her', 'child', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(results[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "    # `encode` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    encoded_sent = tokenizer.encode(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "\n",
    "                        # This function also supports truncation and conversion\n",
    "                        # to pytorch tensors, but we need to do padding, so we\n",
    "                        # can't use these features :( .\n",
    "                        #max_length = 128,          # Truncate all sentences.\n",
    "                        #return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.\n",
    "    input_ids.append(encoded_sent)\n",
    "\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', sentences[0])\n",
    "print('Token IDs:', input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from progressbar import ProgressBar\n",
    "pbar = ProgressBar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Value out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-156-277acdb87fcd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtimex_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:/Users/itsma/Documents/Capstone project/DS5500-capstone/train_data/\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".xml\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mfile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:/Users/itsma/Documents/Capstone project/DS5500-capstone/train_data/\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\progressbar\\progressbar.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    152\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrval\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    155\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\progressbar\\progressbar.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    248\u001b[0m                 and not 0 <= value <= self.maxval):\n\u001b[0;32m    249\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Value out of range'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Value out of range"
     ]
    }
   ],
   "source": [
    "input_ids = []\n",
    "event_labels = []\n",
    "timex_labels = []\n",
    "\n",
    "for file in pbar(os.listdir(\"C:/Users/itsma/Documents/Capstone project/DS5500-capstone/train_data/\")):\n",
    "    if file.endswith(\".xml\"):\n",
    "        file_name = os.path.join(\"C:/Users/itsma/Documents/Capstone project/DS5500-capstone/train_data/\", file)\n",
    "        tree = ET.parse(file_name)\n",
    "        root = tree.getroot()\n",
    "        discharge_note = DischargeNote(root,1)\n",
    "        results = discharge_note.process_note()\n",
    "        \n",
    "        sample_length = len(results[0])\n",
    "        \n",
    "        for index in range(sample_length):\n",
    "            input_ids.append(results[0][index])\n",
    "            event_labels.append(results[2][index])\n",
    "            timex_labels.append(results[3][index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10507"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig.from_pretrained('C:/Users/itsma/Documents/BERT_models/NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12')\n",
    "config.output_hidden_states = True\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('C:/Users/itsma/Documents/BERT_models/NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12')\n",
    "bert_model = BertModel.from_pretrained(\"C:/Users/itsma/Documents/BERT_models/NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12\",config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_input1 = \"I love my Dog.He is Cute.\" \n",
    "bert_input2 = \"He is Cute.\"\n",
    "encodings1 = bert_tokenizer.encode(bert_input1,add_special_tokens = True)\n",
    "input_ids1 = torch.tensor(encodings1).unsqueeze(0)  \n",
    "encodings2 = bert_tokenizer.encode(bert_input2,add_special_tokens = True)\n",
    "input_ids2 = torch.tensor(encodings2).unsqueeze(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  1045,  2293,  2026,  3899,  1012,  2002,  2003, 10140,  1012,\n",
       "           102]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 1045, 2293, 2026, 3899, 1012, 2002, 2003, 10140, 1012, 102]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  2002,  2003, 10140,  1012,   102]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "[0] * 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_list = [1,2,3,4,5,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_list[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
